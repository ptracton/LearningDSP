#+LaTex_HEADER: \usepackage{listings}

* Statistics, Probability and Noise
** Signals and Graph Terminology
*** Definitions
- *Signal* is how one parameter is related to another parameter
- *Continuous Signal* is if BOTH parameters can assume a continuous range
- *Discrete Signal* is if BOTH parameters are quantized in some manner
- *Time Domain* is if the X axis (the independent variable) is time
- *Frequency Domain* is if the X axis (the independent variable) is frequency
*** Concepts
- The two parameters of a signal are not interchangeable
- The parameter on the Y axis is a function of the one on the X axis
- Mathematicians tend to do 1-N, everyone else does 0-(N-1)
** Mean and Standard Deviation
*** Mean
- *Mean* \mu is the average of the signal.  Add all samples together and divide by N. In electronics this is the DC (direct current) value.

$$\mu = \frac{1}{N}\sum_{i=1}^{N-1}x_{i}$$

#+BEGIN_SRC python :session :results output
import random
samples = random.sample(range(1, 101), 20)
def Mean(data=None):
    """
    Calculate the mean of a list of values.
    """
    if data is None:
        return
    mean = 0
    for x in data:
       mean = mean + x
       mean = mean/len(data)
    return mean

samples

Mean(samples)

#+END_SRC

#+RESULTS: 
: [15, 18, 86, 21, 75, 45, 61, 19, 99, 36, 34, 38, 58, 51, 59, 48, 10, 96, 32, 1]
: 0

*** Standard Deviation and Variance
- *Average Deviation* is not commonly used.  Sums up all the deviations, from the mean, for each sample and divided by the number of samples.  Use absolute values for deviation otherwise differences could cancel out.
- *Standard Deviation* averages the power.  This is the AC portion of the signal.  
$$\sigma = \sqrt{\frac{1}{N-1}\sum_{i=1}^{N-1} (x_{i} - \mu)^{2}}$$ 

#+BEGIN_SRC python :session :results output
import math
def StandardDeviation(samples):
    """
    Calculate the standard deviation of a list of values.
    Uses the Mean method from previous examples
    """
    mean = Mean(samples)
    std = 0.0
    for x in samples:
        std = std + math.pow((x - mean), 2)
        std = std / (len(samples) - 1)
        std = math.sqrt(std)
    return std

samples

StandardDeviation(samples)
#+END_SRC

#+RESULTS: 
: [78, 83, 38, 63, 35, 55, 82, 99, 7, 43]
: 12.849613134796984



- *Variance*  is commonly used in statistics.  Notice variance and standard deviation both divide by N-1, not N!  
$$\sigma^{2} = \frac{1}{N-1}\sum_{i=1}^{N-1} (x_{i} - \mu)^{2}$$

#+BEGIN_SRC python :session :results output

def Variance(samples):
    """
    Calculate the variance of a list of values.
    """
    return math.pow(StandardDeviation(samples), 2)

samples

Variance(samples)
#+END_SRC

#+RESULTS: 
: [78, 83, 38, 63, 35, 55, 82, 99, 7, 43]
: 165.11255771394718

- *Root Mean Square (rms)* measures both the AC and DC components.
$$x_{rms} = \sqrt{\frac{1}{N}\sum_{i=0}^{N-1} (x_{i})^{2}}$$ 

#+BEGIN_SRC python :session :results output
def RootMeanSquare(samples):
    """
    Calculate the Root Mean Square of an input list
    """
    rms = 0
    N = len(samples)
    for x in range(N-1):
        square = samples[x] * samples[x]
    divide = square/N
    rms = math.sqrt(divide)
    return rms

samples

RootMeanSquare(samples)
#+END_SRC

#+RESULTS:
: [63, 48, 33, 13, 75, 34, 44, 99, 27, 20, 30, 32, 85, 77, 55, 70, 21, 86, 45, 67]
: 10.04987562112089

*** Running Statistics
- *Running Statistics* is often needed.  In this situation we want to recompute mean and standard deviation of new signal added in without redoing all of the calculations

$$
\sigma^{2} = \frac{1}{N-1} ( \sum_{i=0}^{N-1}(x_{i})^2 - \frac{1}{N}(\sum_{i=0}^{N-1} x_{i})^2)
$$


#+BEGIN_SRC python :session :results output
def RunningStatistics(samples):
    """
    Calculate the mean, variance and std while running through a list of
    values. The self.samples list should be set when instantiating
    this instance.
    """
    mean = 0
    variance = 0
    std = 0
    temp_sum = 0
    sum_squares = 0
    N = len(samples)
    for x in samples:
        temp_sum = temp_sum + x
        sum_squares = sum_squares + math.pow(x, 2)
        mean = temp_sum/N
        variance = (sum_squares - (math.pow(temp_sum, 2)/N)) / (N - 1)
        std = math.sqrt(variance)
    return mean, variance, std    

samples

RunningStatistics(samples)
#+END_SRC

#+RESULTS:
: [92, 65, 22, 1, 39, 57, 73, 28, 95, 21]
: (49.3, 1024.2333333333331, 32.003645625667914)

- In some situations mean decribes what is being measured and standard deviation measures noise
- *Signal to Noise Ration (SNR)* is a comparison of mean to standard deviation

$$
SNR = \frac{\mu}{\sigma}
$$
#+BEGIN_SRC python :session :results output
def SNR(samples):
    """
    Calculate the Signal to Noise Ratio 
    """
    SNR = Mean(samples)/StandardDeviation(samples)
    return SNR

samples

SNR(samples)
#+END_SRC

#+RESULTS:
: [100, 33, 43, 22, 72, 5, 6, 46, 95, 48]
: 0.4086576025773104

- *Coefficient of Variance (CV)* is the standard deviation divided by the mean and multiplied by 100%.

$$
CV = \frac{\sigma}{\mu} * 100\%
$$
#+BEGIN_SRC python :session :results output

def CV(samples):
    """
    Calculate the Signal to Coefficient of Variation
    """
    CV = (StandardDeviation(samples)/Mean(samples)) * 100
    return CV

samples

CV(samples)
#+END_SRC

#+RESULTS:
: [10, 74, 92, 51, 76, 25, 52, 22, 9, 49, 61, 40, 62, 28, 87, 63, 26, 81, 71, 3]
: Traceback (most recent call last):
:   File "<stdin>", line 1, in <module>
:   File "<stdin>", line 5, in CV
: ZeroDivisionError: float division by zero

- High SNR and Low CV is a good signal!

** Signal vs. Underlying Process
- *Statistics* is the science of interpreting numerical data 
- *Probability* is used in DSP to understand the process that generated the signals
- *Statistical Variation or Fluctuation or Noise* is random irregularity found in actual data
- *Typical Error* is the standard deviation over the square root of the number of samples.  For small N, expect a large error. As N grows larger the error should be shrinking.
$$
Typical Error = \frac{\sigma}{N^\frac{1}{2}}
$$


- *Strong Law of Large Numbers* guarantees that the error becomes zero as N approaches infinity.
- The Standard Deviation equation measures the value of the underlying process, not the actual signal.  Divide through by N to get the value of the signal.
- *Non Stationary* processes that change their underlying behavior.  This causes a slowly changing mean and standard deviation.  
** The Histogram, PMF and PDF
- *Histogram* displays the number of samples there are in the signal at this value or range of values.  
   - This is formed from the acquired signal and works on a finite number of samples.
   - Only operates on discrete data
   - If the number of levels a signal can take are larger than the number of samples, this is a problem!  This happens all the time if you use floating point numbers.

#+NAME: python-histogram
#+BEGIN_SRC python  :results file
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

mu, sigma = 100, 15
x = mu + sigma * np.random.randn(10000)

# the histogram of the data
n, bins, patches = plt.hist(x, 50, normed=1, facecolor='green', alpha=0.75)

# add a 'best fit' line
y = mlab.normpdf(bins, mu, sigma)
l = plt.plot(bins, y, 'r--', linewidth=1)

foo = plt.xlabel('Smarts')
foo = plt.ylabel('Probability')
foo = plt.title(r'$\mathrm{Histogram\ of\ IQ:}\ \mu=100,\ \sigma=15$')
foo = plt.axis([40, 160, 0, 0.03])
foo = plt.grid(True)
foo = plt.savefig('../Notes/histogram.png', bbox_inches='tight')
#+END_SRC

#+RESULTS: python-histogram

#+CAPTION: Histogram graph example
#+NAME:   fig:HISTOGRAM
[[file:../Notes/histogram.png]]

Code and example lifted from [[http://matplotlib.org/1.2.1/examples/pylab_examples/histogram_demo.html][Matplotlib Examples]]

- *Probability Mass Function (PMF)* is the corresponding curve for the underlying process.  
  - This is what the histogram would be with infinite samples.  This means the pmf must be between 0 and 1 with the sum of all values to be equal to 1.  
  - This describes the /probability/ that a certain value will be generated.
  - Only operates on discrete data

- *Probability Density Functions (PDF)*  is to continuous signals what the probability mass function is to discrete signals.
  - The vertical axis is in units of *probability density*
  - Probability is calculated by multiplying the probability density by the range of values
  - If the PDF is not constant over a range, the multiplication becomes an integral.

- *Binning* is done by creating arbitrary ranges for the values to be counted in. These are the bins.  The value of the bin is the number of samples in that range.
  - Too many bins makes it difficult to estimate the amplitude of the PMF
  - Too few bins makes it difficult to estimate the PMF in the horizontal direction
  - The number of bins is a trade off of the resolution in X or Y directions.

** The Normal Distribution
- *Normal Distribution or Gaussian Distribution* is the bell shaped PDF formed from random signals.
  + The basic shape of the curve is generated from the /negative squared exponent/

  + You can convert the raw curve with the mean (\mu) and standard deviation (\sigma).  
  + You also need to normalize the equation so that the are under the curve is equal to 1.
  + The mean (\mu) centers the curve over a particular value.
  + The standard deviation (\sigma) controls the width of the bell shape
  + The sharp drop indicates that the extremes are very rare

$$
y(x) = e ^{-x^{2}}
$$

$$
P(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{-(x-\mu)^{\frac{2}{2\sigma^2}}}
$$

- *Cumulative Distribution Function (CDF)* is the integral of the PDF.  The Gaussian can not be easily integrated.  Numerical methods are required.  The symbol \Phi(x) is used for this.
** Digital Noise Generation
- *Randonm Number Generation* is the heart of digital noise generator.
- *Central Limit Theorem* the sum of random numbers become normally distributed as more and more random samples are added together.  
   - The resulting PDF becomes Gaussian.
   - X is a normally distributed random number.  Requires 2 random numers $R_1$ and $R_2$
$$
X = (-2 logR_{1})^{\frac{1}{2}}cos(2\pi R_{2})
$$

   - *Seed* is the number that the random number generator starts with.  The following equation generates the randomd number based off of the seed.  This allows us to replay a sequence of random numbers.

$$
R = (aS+b) modulo c
$$

   - *Psuedo-random* numbers generated this way are not absolutely random since they are based on the previous  value.
** Precision and Accuracy
- *Accuracy* is hitting your goal
   - difference between mean and the actual value
   - Measure of calibration
   - If calibration corrects the error it was an accuracy issue
- *Precision* is performing the same way repeatedly.  
   - Poor precision results from random errors.
   - Width of histogram
   - Measure of random noise
   - Poor accuracy results from systemic errors
   - If averaging successive reading provide a better measurement then it was a precision error
